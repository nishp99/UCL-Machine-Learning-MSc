{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part IV (30 pts in total)\n",
        "\n",
        "---\n",
        "\n",
        "**SN:** 21146187\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *22nd March, 2022,*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part4.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "#### Q.1: You will implement a number of off-policy multi-step return estimates, and answer questions about their accuracy.\n",
        "\n",
        "#### Q.2: You will be looking at other, TD-like, updates to learn the value function. You will be asked to investigate different properties of these: e.g. convergence properties, variance of updates. This is akin to a typical analysis one would undertaken when proposing a new update rule to learn value functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1p0fpbxQLyn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thswfgXU_p05"
      },
      "source": [
        "## Q1 [11 points total]\n",
        "For many concrete algorithms, we need to combine multi-step updates with off-policy corrections.  The multi-step updates are necessary for efficient learning, while the off-policy corrections are necessary to learn about multiple things at once, or to correct for a distribution mismatch (e.g., when trying to perform a policy-gradient update from logged data).\n",
        "\n",
        "In this section, you will implement various different returns with off-policy corrections.  The next cell has two examples *without* corrections.  These examples compute equivalent returns, but compute those returns in different ways.  These are provided as reference implementations to help you.\n",
        "\n",
        "Note that the implementations both allow for immediate bootstrapping on the current state value. This is unconventional (most literature only allows the first bootstrapping to happen after the first step), but we will use this convention in all implementations below for consistency. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHNH35SZYHBu"
      },
      "outputs": [],
      "source": [
        "#@title On-policy return computations\n",
        "\n",
        "def on_policy(observations, actions, pi, mu, rewards, discounts,\n",
        "              trace_parameter, v_fn):\n",
        "  \"\"\"Compute on-policy return recursively.\"\"\"\n",
        "  del mu  # The policy probabilities are ignored by this function\n",
        "  del pi\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  G = np.zeros((T,))\n",
        "  # recurse backwards to calculate returns\n",
        "  for t in reversed(range(T)):\n",
        "    # There are T+1 observations, but only T rewards, and the indexing here\n",
        "    # for the rewards is off by one compared to the indexing in the slides\n",
        "    # and in Sutton & Barto.  In other words, r[t] == R_{t+1}.\n",
        "    if t == T - 1:\n",
        "      G[t] = r[t] + d[t]*v[t + 1]\n",
        "    else:\n",
        "      G[t] = r[t] + d[t]*((1 - l)*v[t + 1] + l*G[t + 1])\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return (1 - l)*v + l*G\n",
        "\n",
        "def on_policy_error_recursion(observations, actions, pi, mu, rewards, discounts,\n",
        "                              trace_parameter, v_fn):\n",
        "  del pi  # The target policy probabilities are ignored by this function\n",
        "  del mu  # The behaviour policy probabilities are ignored by this function\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  errors = np.zeros((T,))\n",
        "    \n",
        "  error = 0.\n",
        "  # recurse backwards to calculate errors\n",
        "  for t in reversed(range(T)):\n",
        "    error = r[t] + d[t]*v[t + 1] - v[t] + d[t]*l*error\n",
        "    errors[t] = error\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return v + l*errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNXhobrYHeiy"
      },
      "source": [
        "### Q 1.1 [5 points]\n",
        "Implement the return functions below and run the cells below that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPzHHrmn5Tm7"
      },
      "outputs": [],
      "source": [
        "def full_importance_sampling(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with full importance-sampling corrections, so that\n",
        "  the return G_t is corrected with the full importance-sampling correction of\n",
        "  the rest of the trajectory.\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  G = np.zeros((T,))\n",
        "  # recurse backwards to calculate returns\n",
        "  for t in reversed(range(T)):\n",
        "    # There are T+1 observations, but only T rewards, and the indexing here\n",
        "    # for the rewards is off by one compared to the indexing in the slides\n",
        "    # and in Sutton & Barto.  In other words, r[t] == R_{t+1}.\n",
        "    if t == T - 1:\n",
        "      G[t] = r[t] + d[t]*v[t + 1]\n",
        "    else:\n",
        "      G[t] = r[t] + d[t]*((1 - l)*v[t + 1] + l*G[t + 1])\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return np.flip(np.cumprod(pi/mu))* ((1 - l)*v + l*G)\n",
        "\n",
        "def per_decision(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with per-decision importance-sampling corrections.\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  G = np.zeros((T,))\n",
        "  # recurse backwards to calculate returns\n",
        "  for t in reversed(range(T)):\n",
        "    # There are T+1 observations, but only T rewards, and the indexing here\n",
        "    # for the rewards is off by one compared to the indexing in the slides\n",
        "    # and in Sutton & Barto.  In other words, r[t] == R_{t+1}.\n",
        "    if t == T - 1:\n",
        "      G[t] = (pi[t]/mu[t])*(r[t] + d[t]*v[t + 1])\n",
        "    else:\n",
        "      G[t] = (pi[t]/mu[t])*(r[t] + d[t]*((1 - l)*v[t + 1] + l*G[t + 1]))\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return (1 - l)*v + l*G\n",
        "\n",
        "def control_variates(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with \n",
        "  1. per-decision importance-sampling corrections, and\n",
        "  2. control variates\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  errors = np.zeros((T,))\n",
        "    \n",
        "  e = 0.\n",
        "  # recurse backwards to calculate errors\n",
        "  for t in reversed(range(T)):\n",
        "    e = (pi[t]/mu[t])*(r[t] + d[t]*v[t + 1] - v[t] + d[t]*l*e)\n",
        "    errors[t] = e\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return v + l*errors\n",
        "\n",
        "def adaptive_bootstrapping(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with \n",
        "  1. per-decision importance-sampling corrections, and\n",
        "  2. control variates, and\n",
        "  3. adaptive bootstrapping.\n",
        "\n",
        "  Implement the adaptive bootstrapping with an *additional* trace parameter\n",
        "  lambda, such that lambda_t = lambda * min(1, 1/rho_t).\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  errors = np.zeros((T,))\n",
        "    \n",
        "  error = 0.\n",
        "  # recurse backwards to calculate errors\n",
        "  for t in reversed(range(T)):\n",
        "    l = np.min([1, mu[t]/pi[t]]) * trace_parameter\n",
        "    error = (pi[t]/mu[t])*(r[t] + d[t]*v[t + 1] - v[t] + d[t]*l*error)\n",
        "    errors[t] = error\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return v + l*errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EEHYK89ANIA"
      },
      "outputs": [],
      "source": [
        "#@title (Run, don't modify) Functions to generate experience, compute values\n",
        "MU_RIGHT = 0.5\n",
        "PI_RIGHT = 0.9\n",
        "NUMBER_OF_STEPS = 5\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "def generate_experience():\n",
        "  r\"\"\"Generate experience trajectories from a tabular tree MDP.\n",
        "\n",
        "  This function will start in state 0, and will then generate actions according\n",
        "  to a uniformly random behaviour policy.  When A_t == 0, the action will be to\n",
        "  the left, with A_t==1, it will be to the right.  The states are nunmbered as\n",
        "  depicted below:\n",
        "          0\n",
        "         / \\\n",
        "        1   2\n",
        "       / \\ / \\\n",
        "      3   4   5\n",
        "         ...\n",
        "  \n",
        "  Args:\n",
        "      number_of_steps: the number of total steps.\n",
        "      p_right: probability of the behaviour to go right.\n",
        "\n",
        "  Returns:\n",
        "      A dictionary with elements:\n",
        "        * observations (number_of_steps + 1 integers): the\n",
        "          observations are just the actual (integer) states\n",
        "        * actions (number_of_steps integers): actions per step\n",
        "        * rewards (number_of_steps scalars): rewards per step\n",
        "        * discounts (number_of_steps scalars): currently always 0.9,\n",
        "          except the last one which is zero\n",
        "        * mu (number_of_steps scalars): probability of selecting each\n",
        "          action according to the behavious policy\n",
        "        * pi (number_of_steps scalars): probability of selecting each\n",
        "          action according to the target policy (here p(1) = 0.9 and\n",
        "          p(0) = 0.1, where a==1 implies we go 'right')\n",
        "  \"\"\"\n",
        "  # generate actions\n",
        "  actions = np.array(np.random.random(NUMBER_OF_STEPS,) < MU_RIGHT,\n",
        "                     dtype=np.int)\n",
        "  s = 0\n",
        "  # compute resulting states\n",
        "  states = np.cumsum(np.arange(1, NUMBER_OF_STEPS + 1) + actions)\n",
        "  states = np.array([0] + list(states))  # add start state\n",
        "\n",
        "  # in this case, observations are just the real states\n",
        "  observations = states\n",
        "\n",
        "  # generate rewards\n",
        "  rewards     = 2.*actions - 1. # -1 for left, +1 for right, \n",
        "  rewards[-1] = np.sum(actions)  # extra final reward for going right\n",
        "    \n",
        "  # compute discounts\n",
        "  discounts     = DISCOUNT * np.ones_like(rewards)\n",
        "  discounts[-1] = 0.  # final transition is terminal, has discount=0\n",
        "\n",
        "  # determine target and behaviour probabilities for the selected actions\n",
        "  pi = np.array([1. - PI_RIGHT, PI_RIGHT])[actions] # Target probabilities\n",
        "  mu = np.array([1. - MU_RIGHT, MU_RIGHT])[actions] # Behaviour probabilities\n",
        "    \n",
        "  return dict(observations=observations,\n",
        "              actions=actions,\n",
        "              pi=pi,\n",
        "              mu=mu,\n",
        "              rewards=rewards,\n",
        "              discounts=discounts)\n",
        "\n",
        "def true_v(s, pi, number_of_steps):\n",
        "  \"\"\"Compute true state value recursively.\"\"\"\n",
        "  depth = int(np.floor((np.sqrt(1 + 8*s) - 1)/2))\n",
        "  position = int(s - depth*(depth+1)/2)\n",
        "  remaining_steps = number_of_steps - depth\n",
        "  final_reward = DISCOUNT**(remaining_steps-1)*(position + pi*remaining_steps)\n",
        "  reward_per_step = pi*(+1) + (1 - pi)*(-1)\n",
        "  discounted_steps = (1 - DISCOUNT**(remaining_steps - 1))/(1 - DISCOUNT)\n",
        "  reward_along_the_way = reward_per_step * discounted_steps\n",
        "  return reward_along_the_way + final_reward\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCFMUmOfRTqZ"
      },
      "outputs": [],
      "source": [
        "#@title Run experiment (don't modify)\n",
        "algs = ['on_policy', 'full_importance_sampling', 'per_decision', 'control_variates', 'adaptive_bootstrapping']\n",
        "\n",
        "# Precompute state values (for efficiency)\n",
        "N = NUMBER_OF_STEPS\n",
        "true_vs = [true_v(s, PI_RIGHT, N) for s in range((N+1)*(N+2)//2)]\n",
        "\n",
        "def random_v(iteration, s):\n",
        "  rng = np.random.RandomState(seed=s + iteration*10000)\n",
        "  return true_vs[s] + rng.normal(loc=0, scale=1.)  # Add fixed random noise \n",
        "\n",
        "def plot_errors(ax, errors):\n",
        "  errors = np.array(errors)\n",
        "  ax.violinplot(np.log10(errors), showextrema=False)\n",
        "  ax.plot(range(1, len(algs)+1), np.log10(errors).T,\n",
        "          '.', color='#667799', ms=7, alpha=0.2)\n",
        "  ax.plot(range(1, len(algs)+1), np.log10(np.mean(errors, axis=0)),\n",
        "          '.', color='#000000', ms=20)\n",
        "  ax.set_yticks(np.arange(-2, 5))\n",
        "  ax.set_yticklabels(10.**np.arange(-2, 5), fontsize=13)\n",
        "  ax.set_ylabel(\"Value error $(v(s_0) - v_{\\\\pi}(s_0))^2$\", fontsize=15)\n",
        "  ax.set_xticks(range(1, len(algs)+1))\n",
        "  ax.set_xticklabels(algs, fontsize=15, rotation=70)\n",
        "  ax.set_ylim(-1, 4)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "errors = []\n",
        "estimates = []\n",
        "v0 = true_vs[0]\n",
        "for iteration in range(1000):\n",
        "  errors.append([])\n",
        "  estimates.append([])\n",
        "  trajectory = generate_experience()\n",
        "  for alg in algs:\n",
        "    estimate = eval(alg)(**trajectory,\n",
        "                        v_fn=lambda s: random_v(iteration, s),\n",
        "                        trace_parameter=0.9)\n",
        "    errors[-1].append((estimate[0] - v0)**2)\n",
        "print(np.mean(errors, axis=0))\n",
        "plot_errors(plt.gca(), errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hlc4jctHHqv"
      },
      "source": [
        "Above, the distributions of mean squared value errors are shown, with the mean as a big black dot and the (1,000) individual return samples as small black dots.\n",
        "\n",
        "### Q 1.2 [3 points]\n",
        "Explain the ranking in terms of value error of the different return estimates.\n",
        "\n",
        "1. Adaptive Bootstrapping\n",
        "2. Control Variates\n",
        "3. Per Decision\n",
        "4. Full Importance Sampling\n",
        "\n",
        "1. In Adaptive Bootstrapping, when the $\\mu$ is too far from the target policy $\\pi$, the value of the trace parameter $\\lambda$ is set to 1, this gives a cutoff in the sum of errors, giving an overall lower variance.\n",
        "\n",
        "2. Using control variates, the error is changed so as to lower the variance. Per decision is also used in conjunction, alleviating the impact of terms that increase variance.\n",
        "\n",
        "3. Per-decision update removes some of the impact of terms that add to the variance.\n",
        "\n",
        "4. Full importance sampling has the largest variance, as it cannot remove the effect of outliers, which cause the square error terms to be large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Uk1d9b4CPc"
      },
      "source": [
        "\n",
        "\n",
        "### Q 1.3 [3 points]\n",
        "Could there be a reason to **not** choose the best return according to this ranking when learning off-policy?  Explain your answer.\n",
        "\n",
        "Yes.\n",
        "\n",
        "If function approximation is being used during this off-policy learning, then bootstrapping should be avoided. This is because the application of function approximation, off-policy learning and bootstrapping form something called the 'deadly triad'. Referring to the danger of instability and divergence from combining these three elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Xn8bDNFm-H"
      },
      "source": [
        "## Q2 [19 points total]\n",
        " Consider a MDP $M = (\\mathcal{S}, \\mathcal{A}, p, r, \\gamma)$ and a behaviour policy $\\mu$. We use policy $\\mu$ to generate trajectories of experience:\n",
        "\\begin{equation*}\n",
        "    (s_{t}, a_{t}, r_{t},s_{t+1}, a_{t+1}, r_{t+1},\\cdots, s_{t+n-1}, a_{t+n-1}, r_{t+n-1}, s_{t+n}, a_{t+n}) \\,.\n",
        "\\end{equation*}\n",
        "Note that this is an $n$-step sequence, starting from time $t$.\n",
        "\n",
        "Given these partial trajectories we consider the following learning problems:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vc8FfT06lYY"
      },
      "source": [
        "### Q2.1 [9 points]\n",
        "Consider a learning update based on the following temporal difference error:\n",
        "$$\\delta_t = R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q(S_{t+2}, a) - q(S_t, A_t)$$\n",
        "\n",
        "Consider updating a tabular action value function with TD.\n",
        "\n",
        "i) Does the resulting value function converge, under any initialisation of the value function? Consider an appropiate learning rate (Robbins–Monro conditions). If so, prove the convergence under infinity number of interactions with this MDP, under fixed behaviour policy $\\mu$ and show its convergence point. If not, show why it diverges. (7 points)\n",
        "\n",
        "ii) Under which conditions, would the above process converge to the optimal value function $q_*$ ? (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H4p8jZj6bGP"
      },
      "source": [
        "i) The action value function is:\n",
        "\n",
        "\n",
        "  $q_{t+1}(S_t,A_t) = q_{t}(S_t,A_t) + \\alpha_t\\delta_t$ \\\\\n",
        "  $q_{t+1}(S_t,A_t) = q_{t}(S_t,A_t) + \\alpha_t(R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q(S_{t+2}, a) - q(S_t, A_t))$ \\\\\n",
        "  $q_{t+1}(S_t,A_t) = (1 - \\alpha_t)q_{t}(S_t, A_t) + \\alpha_t(R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q_{t}(S_{t+2}, a))$\n",
        "\n",
        "  Defining: An action value function with arbitrary initialisation as $q^{(2)}$, and defining action value function at iteration $t$ that is updated according to the temporal difference error as $q^{(1)}_t$. We can then define:\n",
        "\n",
        "  $\\Delta_t(S_t, A_t ) = q^{(1)}_{t}(S_t, A_t) - q^{(2)}(S_t, A_t)$\n",
        "\n",
        "  By subracting $q^{(2)}$ from the value function update, we obtain:\n",
        "$$\\Delta_{t+1}(S_t, A_t) = (1 - \\alpha_t)\\Delta_{t}(S_t, A_t) + \\alpha_t(R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q^{(1)}_{t}(S_{t+2}, a) - q^{(2)}(S_t, A_t))$$\n",
        "\n",
        "This is of the form:\n",
        "$$\\Delta_{t+1}(S_t, A_t) = (1 - \\alpha_t)\\Delta_{t}(S_t, A_t) + \\alpha_tF_t(S_t, A_t)$$ \n",
        "\n",
        "where $F_t(S_t, A_t) = R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q^{(1)}_{t}(S_{t+2}, a) - q^{(2)}(S_t, A_t)$.\n",
        "\n",
        "In http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf, Theorem 1 states that a random process of this form converges to 0 under the following assumptions: \\\\\n",
        "1. $0 \\leq \\alpha_t \\leq 1$, $\\sum_t \\alpha_t = \\infty$, and $\\sum_t \\alpha^2_t < \\infty$\n",
        "2. $|| E[F_t(S_t, A_t)|P_t] ||_w < \\gamma||\\Delta(S_t, A_t)||_w$, where $\\gamma \\in (0, 1)$\n",
        "3. $Var[F_t(S_t, A_t)|P_t] \\le C(1 + ||\\Delta_t(S_t, A_t)||_w)^2$, where $C$ is some constant\n",
        "\n",
        "Where $P_t = \\{ \\Delta_t(S_t, A_t), \\Delta_{t-1}(S_{t-1}, A_{t-1}), ..., F_{t-1}(S_{t-1}, A_{t-1}), ..., \\alpha_{t-1}\\}$ and $|| .||_w$ refers to some weighted maxium norm\n",
        "\n",
        "We now show that conditions 2 and 3 hold:\n",
        "Defining $\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$,\n",
        "\\begin{equation} \n",
        "  \\mathcal{T}q(s_t,a_t) = \\mathbb{E}_{a_{t+1} \\sim \\mu(\\cdot |s_{t+1}), s_{t+1} \\sim p(\\cdot | s_t,a_t), s_{t+2} \\sim p(\\cdot | s_t+1,a_t+1)}\\left[ R(s_t,a_t) +  \\gamma R(s_{t+1}, a_{t+1}) + \\gamma^2 \\max_a q(s_{t+2}, a)\\right]\n",
        "\\end{equation} \\\\\n",
        "\n",
        "We find that:\n",
        "\n",
        "$$|\\mathcal{T}q -\\mathcal{T}u| \\leq \\gamma^2 \\max_a |\\mathbb{E}[q-u] | $$  \n",
        "\n",
        "Thus we get:\n",
        "\n",
        "$$\\|\\mathcal{T}q -\\mathcal{T}u\\|_{\\infty} \\leq \\gamma^2 \\max_a \\|q-u\\|_{\\infty} $$ \n",
        "\n",
        "This is a $\\gamma^2$-contraction mapping, and by the Banach fixed point theorem, it has a unique fixed point. call it $q^{*}$\n",
        "\n",
        "Let us set $q^{(2)} = q^{*}$ \\\\\n",
        "\n",
        "$$\\mathbb{E}[F_t(S_t, A_t)|P_t] = \\mathbb{E}[R(s_t,a_t) +  \\gamma R(s_{t+1}, a_{t+1}) + \\gamma^2 \\max_a q(s_{t+2}, a) - q^* ]$$ \\\\\n",
        "$$ = (\\mathcal{T}q)(S_t,A_t)) - q^*(S_t,A_t)$$ \\\\\n",
        "\n",
        "using $\\mathcal{T}q^* = q^*$\n",
        "$$ = (\\mathcal{T}q)(S_t,A_t)) - (\\mathcal{T}q^*)(S_t,A_t))$$ \\\\\n",
        "$$ \\Rightarrow \\| \\mathbb{E}[F_t(S_t, A_t)|P_t]\\|_{\\infty}  \\leq  \\gamma^2 \\| q_t - q^* \\|_{\\infty}$$, hence assumption 2 is satisfied.\n",
        "\n",
        "Similarly, it can be shown that $Var[F_t(S_t, A_t)|P_t] \\le C(1 + ||\\Delta_t(S_t, A_t)||_w)^2$. So $\\Delta_t$ converges to 0, i.e. $q_t$ converges to $q^*$.\n",
        "\n",
        "ii) Under what conditions will this process converge to the optimal value function, (i.e. when is $q^*$ optimal?): If $\\mu$ isn't fixed, and the algorithm is greedy in the limit of infinite exploration, it will converge to the optimal value function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfIqVjrP6QI-"
      },
      "source": [
        "### Q2.2 [10 points]\n",
        "\n",
        "Consider the same questions now for the following temporal difference error\n",
        "\\begin{equation}\n",
        "        \\delta_t = r(S_{t},A_{t}) + \\gamma \\frac{\\pi(A_{t+1}|S_{t+1})}{\\mu(A_{t+1}|S_{t+1})} \\left[ r(S_{t+1},A_{t+1}) + \\gamma \\max_{a} q(S_{t+2},a) \\right] - q(S_t, A_t)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\pi(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$ and consider the behaviour policy to be either:\n",
        "\n",
        "  a. $\\mu(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$,\n",
        "  \n",
        "  b. $\\mu(a|s) = \\frac{1}{|\\mathcal{A}|}$ (uniformly random policy).\n",
        "\n",
        "Answer the below two questions for **both choices** of the behaviour policy $\\mu$:\n",
        "* i)  Does updating a tabular action value function with this TD error converge to the optimal value function $q_*$? Consider an appropiate learning rate (Robbins–Monro conditions). If so, prove this convergence under infinity number of interaction with this MDP, under behaviour policy $\\mu$. If not, show why it diverges or alternatively convergence to a different solution. (4 points)\n",
        "\n",
        "a) Following the same process as in Q2.1, we obtain an almost identical equation, showing that the function will converge for all states. However, due to the policy $\\mu(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$, this policy will not explore all states infinitely often, so it is not guaranteed to converge optimally. \\\\\n",
        "\n",
        "b) No convergence\n",
        "\n",
        "* ii) How does the variance of this update compare to the one induced by the error in Q5.1? (3 points). \n",
        "\n",
        "$$Var(\\delta_t) = Var(R(S_t, A_t)) + Var(\\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q(S_{t+2}, a) - q(S_t, A_t))$$\n",
        "\n",
        "$$Var(\\delta_t) = Var(r(S_{t},A_{t})) + (\\frac{\\pi(A_{t+1}|S_{t+1})}{\\mu(A_{t+1}|S_{t+1})})^2 Var(\\gamma  \\left[ R(S_{t+1},A_{t+1}) + \\gamma \\max_{a} q(S_{t+2},a) \\right] - q(S_t, A_t))$$ \\\\\n",
        "\n",
        "The difference between the two variances is the terms involving a prefactor of $(\\frac{\\pi(A_{t+1}|S_{t+1})}{\\mu(A_{t+1}|S_{t+1})})^2$. Given that this prefactor can never be less than 1, the variance is increased.\n",
        "\n",
        "* iii) Can you propose a different behaviour policy that achieves a lower variance than any of the choices we considered for $\\mu$? Prove that your behaviour policy achieve this. Argue why, if that is not possible. (3 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGqQJRzD6Tww"
      },
      "source": [
        "*Answer here:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wesi6S866Lyq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "21146187_RL_part4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}