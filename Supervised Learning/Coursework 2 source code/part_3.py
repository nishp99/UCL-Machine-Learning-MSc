# -*- coding: utf-8 -*-
"""Part 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1thMBRKxB7NHq1sdbyMnsqfVIUDVLdASs
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import re
import math

"""
generates m samples of n dimensional data
"""
def generate_data(n,m):
  values = np.array([-1,1])
  if m == 1:
    return (torch.tensor(np.random.choice(values, n)).float()).unsqueeze(0)
  else:
    return (torch.tensor(np.random.choice(values, (m,n))).float())

test = generate_data(10,50)
print(test.size())

"""
generates m samples of n dimensional data (1's and 0's for the winnow)
"""
def generate_winnow_data(n,m):
  values = np.array([0,1])
  if m == 1:
    return (torch.tensor(np.random.choice(values, n)).float()).unsqueeze(0)
  else:
    return (torch.tensor(np.random.choice(values, (m,n))).float())

"""
least squares class, that uses m training points of n-dimensional data
classify method returns the generalisation error on some text set X
"""
class least_squares():
  def __init__(self,n,m):
    X_train = generate_data(n, m).float()
    y_train = X_train[:,0]
    self.weight =  torch.linalg.lstsq(X_train, y_train).solution

  def classify(self, X, Y):
    #a = torch.sign(X @ self.weight.t()
    correct = torch.eq(Y, torch.sign(X @ self.weight.t()))
    return torch.sum(correct)/X.size()[0]

iterator = [2*i+1 for i in range(50)]

"""
Finds the average sample complexity of least squares for multiple dimensions n
generates a new test set of data for each n
M_s contains the mean sample complexities
"""

M_s = []
stds = []
for n in iterator:
  m_s = []
  test_X = generate_data(n,100)
  test_Y = test_X[:,0]
  for _ in range(50):
    accuracy = 0
    m = 1
    while accuracy < 0.9:
      ls = least_squares(n,m)
      accuracy = ls.classify(test_X, test_Y)
      m += 1
    m_s.append(m-1)
  M_s.append(torch.mean(torch.tensor(m_s).float()))
  stds.append(torch.std(torch.tensor(m_s).float()))

plt.errorbar(iterator,M_s, yerr = stds)
plt.xlabel('n')
plt.ylabel('m')
plt.title('least squares sample complexity')

"""
perceptron class, that uses m training points of n-dimensional data
classify method returns the generalisation error on some text set X
"""

class perceptron():
  def __init__(self, n, m):
    self.train_X = generate_data(n,m)
    self.train_Y = self.train_X[:,0]
    self.weight = torch.zeros(n)
  
  """
  classifies the ith training point
  """
  def classify(self, i):
    return torch.sign(self.weight @ self.train_X[i])
  
  """
  checks is the classification of the ith point is correct
  """
  def mistake(self,i):
    return self.classify(i) != self.train_Y[i]

  """
  performs the perceptron update on the weight using the 
  ith point
  """
  def update(self, i):
    self.weight = self.weight + self.train_Y[i]*self.train_X[i]
  
  """
  trains the perceptron by looping over the training data
  20 times
  """
  def train(self):
    for epoch in range(20):
      for i in range(self.train_Y.size()[0]):
        if self.mistake(i):
          self.update(i)

  """
  tests the perceptron on some test set X
  """
  def test(self, X, Y):
    classes = torch.sign(self.weight @ X.t())
    return torch.sum(torch.eq(Y,classes))/Y.size()[0]

"""
Finds the average sample complexity of perceptron for multiple dimensions n
generates a new test set of data for each n
percep_Ms contains the mean sample complexities
"""

percep_Ms = []
percep_stds = []

for n in iterator:
  percep_m_s = []
  percep_test_X = generate_data(n,100)
  percep_test_Y = percep_test_X[:,0]
  for _ in range(50):
    accuracy = 0
    m = 1
    while accuracy < 0.9:
      percept = perceptron(n,m)
      percept.train()
      accuracy = percept.test(percep_test_X, percep_test_Y)
      m += 1
    percep_m_s.append(m-1)
  percep_Ms.append(torch.mean(torch.tensor(percep_m_s).float()))
  percep_stds.append(torch.std(torch.tensor(percep_m_s).float()))

plt.errorbar(iterator,percep_Ms, yerr = percep_stds)
plt.xlabel('n')
plt.ylabel('m')
plt.title('Perceptron sample complexity')

"""
winnow class, that uses m training points of n-dimensional data
classify method returns the generalisation error on some text set X
"""
class winnow():
  def __init__(self, n, m):
    self.n = n
    self.m = m
    self.X_train = generate_winnow_data(n, m)
    self.Y_train = self.X_train[:,0]
    self.weight = torch.ones(n)

  """
  classifies the ith training point
  """
  def predict(self, i):
    return torch.heaviside(self.weight @ self.X_train[i] - self.n*torch.ones(1), torch.ones(1))
  
  """
  checks is the classification of the ith point is correct
  """
  def mistake(self, i, y_t):
    return y_t != self.Y_train[i]
  
  """
  performs the perceptron update on the weight using the 
  ith point
  """
  def update(self, i, y_t):
    self.weight = self.weight * torch.exp(math.log(2) * (self.Y_train[i] - y_t) * self.X_train[i])
  
  """
  trains the perceptron by looping over the training data
  20 times
  """
  def train(self):
    for _ in range(20):
      for i in range(self.Y_train.size()[0]):
        y_t = self.predict(i)
        if self.mistake(i, y_t):
          self.update(i, y_t)

  """
  tests the perceptron on some test set X
  """
  def test(self, X, Y):
    prediction = torch.heaviside(self.weight @ X.t() - self.n*torch.ones_like(Y), torch.ones_like(Y))
    return torch.sum(torch.eq(Y,prediction))/Y.size()[0]

"""
Finds the average sample complexity of winnow for multiple dimensions n
generates a new test set of data for each n
win_Ms contains the mean sample complexities
"""

win_Ms = []
win_stds = []

for n in iterator:
  win_m_s = []
  win_test_X = generate_winnow_data(n,200)
  win_test_Y = win_test_X[:,0]
  for _ in range(150):
    accuracy = 0
    m = 1
    while accuracy < 0.9:
      win = winnow(n,m)
      win.train()
      accuracy = win.test(win_test_X, win_test_Y)
      m += 1
    win_m_s.append(m-1)
  win_Ms.append(torch.mean(torch.tensor(win_m_s).float()))
  win_stds.append(torch.std(torch.tensor(win_m_s).float()))

plt.errorbar(iterator,win_Ms, yerr = win_stds)
plt.xlabel('n')
plt.ylabel('m')
plt.title('WINNOW sample complexity')

"""
1-NN class, that uses m training points of n-dimensional data
classify method returns the generalisation error on some text set X
"""
class one_NN():
  def __init__(self, n, m):
    self.n = n
    self.m = m
    self.X_train = generate_data(self.n, self.m)
    self.Y_train = self.X_train[:,0]

  """
  computes distance from all points in training set to each other
  """
  def distance(self, X):
    a = torch.cdist(self.X_train, X)
    return a

  """
  returns the indices of the data points that give the minimum distance
  for each training point
  """
  def indices(self, X):
    a = torch.argmin(self.distance(X), dim = 0)
    return a
  
  """
  tests on some test data X
  """
  def test(self, X, Y):
    index = self.indices(X)
    predictions = torch.empty(index.size())
    for i in range(index.size()[0]):
      predictions[i] = self.Y_train[index[i]]
    return torch.sum(torch.eq(predictions, Y))/Y.size()[0]

"""
Finds the average sample complexity of 1-NN for multiple dimensions n
generates a new test set of data for each n
nn_Ms contains the mean sample complexities
"""
nn_Ms = []
nn_stds = []

for n in iterator:
  nn_m_s = []
  nn_test_X = generate_data(n,100)
  nn_test_Y = nn_test_X[:,0]
  for _ in range(10):
    accuracy = 0
    m = 1
    while accuracy < 0.9:
      nn = one_NN(n,m)
      accuracy = nn.test(nn_test_X, nn_test_Y)
      m += 1
    nn_m_s.append(m-1)
  nn_Ms.append(torch.mean(torch.tensor(nn_m_s).float()))
  nn_stds.append(torch.std(torch.tensor(nn_m_s).float()))

plt.errorbar(2*np.arange(30), nn_Ms[:30], yerr = nn_stds[:30])
plt.xlabel('n')
plt.ylabel('m')
plt.title('1-NN sample complexity')