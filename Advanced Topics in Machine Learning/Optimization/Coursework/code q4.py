# -*- coding: utf-8 -*-
"""optimization 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10rgYof4vDwjhXT0YHVPAXSwAgceFdGip
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from scipy import linalg as scplinalg
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib.colors as cl

import random

# increase font size for more readable plots
fontsize = 18
plt.rcParams.update({'axes.labelsize': fontsize,
                     'font.size': fontsize,
                     'legend.fontsize': fontsize,
                     'xtick.labelsize': fontsize - 2,
                     'ytick.labelsize': fontsize - 2})

# %matplotlib inline

from sklearn.datasets import make_moons
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)
y = 2*y - 1

from sklearn.metrics.pairwise import euclidean_distances

"""
creates the kernel matrix with parameter sigma
"""
def kernel(sigma):
  K = euclidean_distances(X,X)**2
  K = np.exp(-K/(2*sigma**2))
  return K

"""
creates the modified kernel matrix, (ith, jth) element = y_iK_(ij)y_j
"""
def kernel_y(sigma):
  return np.diag(y) @ kernel(sigma) @ np.diag(y)

"""
computes the proximal operator for a 1-D value. g is taken to be the indicator
function for the for the interval [0, 1/n*lambda]
"""
def prox_1D(lamb, c):
  a = np.reciprocal(200*lamb)

  if c > a:
    return a
  if c < 0:
    return 0
  else:
    return c

"""
defines the proximal operator over arbitrary dimension. g is taken to be the 
indicator function for the hypercube [0, 1/n*lambda]^n
"""

def prox_nD(lamb, c):
  a = np.reciprocal(200*lamb)
  c = np.where(c > a, a , c)
  c = np.where(c < 0, 0, c)
  return c

"""
computes the dual objective function
"""
def dual_objective(alpha, K):
  return (K @ alpha) @ alpha/2 - np.sum(alpha)

"""
computes the gradient of objective function at alpha
"""
def grad_objective(alpha, K):
  return K @ alpha - 1

"""
creates the kernel, we use sigma =2
"""
K = kernel(0.5)

"""
creates the y-modified kernel
"""
K_y = kernel_y(0.5)

"""
the frobenius norm of K_y is calculated below, this defines the 
Lipschitz constant of the dual objective function

the largest eigenvalue is also calculated below, but this gives a larger bound
on gamma
"""

K_norm = np.linalg.norm(K_y, ord = 'fro')
K_norm_2 = np.linalg.norm(K_y, ord = 2)
print(K_norm)
print(K_norm_2)
"""
the reciprocal of the norm is taken to give an upper limit 
on gamma
"""

max_gamma = np.reciprocal(K_norm)
print(max_gamma)
print(np.reciprocal(K_norm_2))

"""
implemetation of FISTA algorithm with lamb = lambda parameter
"""
def FISTA(gamma, lamb, N_iter):
  #initialise starting values of alpha, y and t
  alpha = np.random.uniform(low=0, high = 0.05, size = (200,))
  y = alpha
  t = 1

  #create list which will store objective function values
  values = [dual_objective(alpha, K_y)]

  for _ in range(N_iter):
    #finds the next value of t
    t_next = (1 + np.sqrt(1 + 4 * t))/2 

    #finds the next value of alpha
    alpha_next = prox_nD(lamb, y - gamma * grad_objective(y, K_y))

    #updates y
    y = alpha_next + (alpha_next-alpha) * (t-1)/t_next

    #updates t and alpha
    t = t_next
    alpha = alpha_next

    values.append(dual_objective(alpha, K_y))
  
  plt.plot(np.arange(N_iter+1), values)
  plt.xlabel('iterations')
  plt.ylabel('dual objective function value')
  plt.title('FISTA')
  return alpha

alph = FISTA(0.02, 0.01, 200)

"""
constructs a random sequence of N_iter values, each sampled
uniformly at random from range(n)
"""
def random_sequence(n, N_iter):
  return np.random.choice(n, (N_iter))

"""
rpcga algorithm, with lamb as the lambda parameter
"""
def rpcga(gamma, lamb, N_iter):
  #set the random sequence of indices
  indices = random_sequence(200,N_iter)

  #define an initial value for alpha
  alpha = np.random.uniform(low=0, high = 0.05, size = (200,))

  #create list which will store objective function values
  values = [dual_objective(alpha, K_y)]

  for i in indices:
    #calculates argument of proximal operator
    c = alpha[i] - gamma * grad_objective(alpha, K_y)[i]
    
    #updates the specified element of alpha
    alpha[i] = prox_1D(lamb, c)

    #adds new value to list of objective function values
    values.append(dual_objective(alpha, K_y))

  plt.plot(np.arange(N_iter+1), values)
  plt.xlabel('iterations')
  plt.ylabel('dual objective function value')
  plt.title('RPCGA')
  return alpha

alph_2 = rpcga(0.06, 0.01, 14000)

"""
creates the decision function that acts on some 2D meshgrid (Psi, Chi)
"""
def decision_function(Psi, Chi, alpha, sigma):
  def classifier(x,b):
    exponent = np.linalg.norm(X-np.array([x,b]), axis = 1)**2 / (2*sigma**2)
    k = np.exp(-exponent)
    a = y * alpha * k
    a = np.sum(a)
    return np.sign(a)
  vectorized = np.vectorize(classifier)
  return vectorized(Psi, Chi)

x_s = np.linspace(-2, 3, 400)
y_s = np.linspace(-1.5, 2, 400)
Psi, Chi = np.meshgrid(x_s,y_s)

a = decision_function(Psi, Chi, alph, 0.5)

cmap_light = cl.ListedColormap(['#FFAAAA', '#AAFFAA',])
cmap_bold = cl.ListedColormap(['#FF0000', '#00FF00'])
plt.pcolormesh(Psi, Chi, a, cmap=cmap_light)
plt.scatter(X[:,0], X[:,1], c = y, cmap = cmap_bold, edgecolor = 'k')
plt.title('FISTA')
plt.show()

b = decision_function(Psi, Chi, alph_2, 0.5)

plt.pcolormesh(Psi, Chi, b, cmap=cmap_light)
plt.scatter(X[:,0], X[:,1], c = y, cmap = cmap_bold, edgecolor = 'k')
plt.title('RPCGA')
plt.show()

print(alph_2)

print(alph)

"""
The following section repeats the above, the only difference is the definition 
proximal operator and objective function. For the following, the <1_n,alpha> 
portion of the dual problem is amalgamated into the 'g' used for the proximal
operator.  
"""

def new_prox_1D(lamb, c):
  a = np.reciprocal(200*lamb)
  if c + 1 > a:
    return a
  if c + 1 < 0:
    return 0
  else:
    return c + 1

def new_prox_nD(lamb, c):
  a = np.reciprocal(200*lamb)
  c = c + 1 
  c = np.where(c > a, a , c)
  c = np.where(c < 0, 0, c)
  return c

def new_dual_objective(alpha, K):
  return (K @ alpha) @ alpha/2

def new_grad_objective(alpha, K):
  return K @ alpha

new_K = kernel(0.2)

new_K_y = kernel_y(0.2)

"""
We use the same kernel as defined previously
"""

def new_FISTA(gamma, lamb, N_iter):
  #initialise starting values of alpha, y and t
  alpha = np.random.uniform(low=0, high = 5, size = (200,))
  y = alpha
  t = 1

  #create list which will store objective function values
  values = [new_dual_objective(alpha, new_K_y)]

  for _ in range(N_iter):
    #finds the next value of t
    t_next = (1 + np.sqrt(1 + 4 * t))/2 

    #finds the next value of alpha
    alpha_next = new_prox_nD(lamb, y - gamma * new_grad_objective(y, new_K_y))

    #updates y
    y = alpha_next + (alpha_next-alpha) * (t-1)/t_next

    #updates t and alpha
    t = t_next
    alpha = alpha_next

    values.append(new_dual_objective(alpha, new_K_y))
  
  plt.plot(np.arange(N_iter+1), values)
  plt.xlabel('iterations')
  plt.ylabel('dual objective function value')
  plt.title('new FISTA')
  return alpha

def new_rpcga(gamma, lamb, N_iter):
  #set the random sequence of indices
  indices = random_sequence(200,N_iter)

  #define an initial value for alpha
  alpha = np.random.uniform(low=0, high = 5, size = (200,))

  #create list which will store objective function values
  values = [new_dual_objective(alpha, new_K_y)]

  for i in indices:
    #calculates argument of proximal operator
    c = alpha[i] - gamma * new_grad_objective(alpha, new_K_y)[i]
    
    #updates the specified element of alpha
    alpha[i] = new_prox_1D(lamb, c)

    #adds new value to list of objective function values
    values.append(new_dual_objective(alpha, new_K_y))

  plt.plot(np.arange(N_iter+1), values)
  plt.xlabel('iterations')
  plt.ylabel('dual objective function value')
  plt.title('new RPCGA')
  return alpha

new_alph = new_FISTA(0.04, 0.001, 10)

new_alph_2 = new_rpcga(0.04, 0.001, 3000)

new_a = decision_function(Psi, Chi, new_alph, 0.2)
new_b = decision_function(Psi, Chi, new_alph_2, 0.2)

plt.pcolormesh(Psi, Chi, new_a, cmap=cmap_light)
plt.scatter(X[:,0], X[:,1], c = y, cmap = cmap_bold, edgecolor = 'k')
plt.title('new FISTA')
plt.show()

plt.pcolormesh(Psi, Chi, new_b, cmap=cmap_light)
plt.scatter(X[:,0], X[:,1], c = y, cmap = cmap_bold, edgecolor = 'k')
plt.title('new RPCGA')
plt.show()

print(new_alph)

print(new_alph_2)

"""
Accuracy tests: we tst the accuracy of the found alpha vectors by
counting the percentage of correct classifications on a training set
"""

train_X, train_y =  make_moons(n_samples=10000, noise=0.05, random_state=0)
train_y = 2*train_y - 1

def accuracy(alpha, sigma):
  predictions = decision_function(train_X[:,0], train_X[:,1], alpha, sigma)
  correct = np.equal(predictions, train_y)
  return 100*np.sum(correct)/np.shape(train_y)[0]

print(accuracy(new_alph, 0.2))

